{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected: True\n",
      "GPU device count: 1\n"
     ]
    }
   ],
   "source": [
    "# basics\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime   # \n",
    "import re\n",
    "\n",
    "# load self-written func\n",
    "sys.path.append(\"func/\")      # add path \n",
    "import helper\n",
    "import prompts\n",
    "\n",
    "# load env variables\n",
    "from dotenv import dotenv_values\n",
    "ENV_VAR = dotenv_values(\"../env/.env\")\n",
    "Gemini_key = ENV_VAR['Gemini_key']\n",
    "HF_key = ENV_VAR['HF_key']\n",
    "\n",
    "# DS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# DL\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # HF models\n",
    "import google.generativeai as genai     # Gemini model\n",
    "print(f\"GPU detected: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU device count: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helper' from 'd:\\\\code\\\\quantization\\\\func\\\\helper.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload module if modified\n",
    "from importlib import reload\n",
    "reload(helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\user\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# get credentials\n",
    "from huggingface_hub import login\n",
    "login(token=HF_key)\n",
    "genai.configure(api_key=Gemini_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get prompt arguments\n",
    "* 30 types of conversation tone\n",
    "* 50 types of restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "List 30 types of most common tone in conversation. Your response should have great variety and contain only the answer. Each tone should be separated by a new line\n",
    "\"\"\"\n",
    "generation_config = genai.GenerationConfig(temperature=1, top_k=32, top_p=0.3)\n",
    "tones_str = gemini.generate_content(input_text, generation_config=generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Assertive',\n",
       " 'Aggressive',\n",
       " 'Authoritative',\n",
       " 'Calm',\n",
       " 'Commanding',\n",
       " 'Confident',\n",
       " 'Conversational',\n",
       " 'Critical',\n",
       " 'Cynical',\n",
       " 'Defensive',\n",
       " 'Demeaning',\n",
       " 'Dismissive',\n",
       " 'Enthusiastic',\n",
       " 'Excited',\n",
       " 'Friendly',\n",
       " 'Humorous',\n",
       " 'Impersonal',\n",
       " 'Indifferent',\n",
       " 'Inquisitive',\n",
       " 'Ironic',\n",
       " 'Judgmental',\n",
       " 'Neutral',\n",
       " 'Polite',\n",
       " 'Sarcastic',\n",
       " 'Serious',\n",
       " 'Submissive',\n",
       " 'Sympathetic',\n",
       " 'Understanding',\n",
       " 'Urgent',\n",
       " 'Poor English']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tones = tones_str.text\n",
    "tones = tones.replace('- ', '')\n",
    "tones = tones.split('\\n')\n",
    "tones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "List 60 types of common restaurants. Your response should have great variety and contain only the answer. Each response should be separated by a new line\n",
    "\"\"\"\n",
    "food_str = gemini.generate_content(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_types = food_str.text\n",
    "food_types = re.sub('\\d+\\.\\s', '', food_types)\n",
    "food_types = food_types.split('\\n')\n",
    "food_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm working on building a chatbot for restaurant recommendations. Can you assist me in generating training data in the following format? \n",
      "Please create 20 conversations, each must contain user input and model outputs, and encapsulated within a Python dictionary. All conversations should be wrapped up into a Python list.\n",
      "\n",
      "Ensure the conversations cover a variety of scenarios, preferences, and inquiries related to dining experiences, focusing on Brewery or Buffet restaurants.\n",
      "The tone of user input should be Aggressive, and the sentence structure in each example should be different, \n",
      "reflecting different users and their unique styles of communication.\n",
      "\n",
      "Remember to use double quotes (\"\") to enclose the text in the training data.\n",
      "\n",
      "Example format:\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"user\": \"Hi there! Wondering if you can help me find a spot for a cozy dinner tonight.\",\n",
      "    \"AI assistant\": \"Of course! For a cozy dinner experience, I recommend [Restaurant Name]. They offer intimate ambiance and a menu filled with comforting dishes.\"\n",
      "  },\n",
      "  {\n",
      "    \"input\": \"Hey, I'm in the mood for something spicy and adventurous. Any ideas?\",\n",
      "    \"output\": \"Spicy and adventurous, huh? You'll love [Restaurant Name]. They specialize in bold flavors and exotic dishes that will tantalize your taste buds.\"\n",
      "  }\n",
      "]\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "input_text = prompts.restaurant_calib_data(tones[1], food_types[3:5])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, Tone: Assertive, Food type: ['American', 'Barbecue']\n",
      "Accumulated time spent: 0.3292 minutes\n",
      "1, Tone: Aggressive, Food type: ['Breakfast', 'Brewery']\n",
      "Accumulated time spent: 0.6882 minutes\n",
      "2, Tone: Authoritative, Food type: ['Buffet', 'Burger joint']\n",
      "Accumulated time spent: 1.0274 minutes\n",
      "3, Tone: Calm, Food type: ['Cafe', 'Chinese']\n",
      "Accumulated time spent: 1.3316 minutes\n",
      "4, Tone: Commanding, Food type: ['Coffee shop', 'Creperie']\n",
      "Accumulated time spent: 1.6456 minutes\n",
      "5, Tone: Confident, Food type: ['Diner', 'Fast food']\n",
      "Accumulated time spent: 1.9313 minutes\n",
      "6, Tone: Conversational, Food type: ['Fine dining', 'Food truck']\n",
      "Accumulated time spent: 2.2476 minutes\n",
      "7, Tone: Critical, Food type: ['Gastropub', 'Greek']\n",
      "Accumulated time spent: 2.5385 minutes\n",
      "8, Tone: Cynical, Food type: ['Hamburger stand', 'Health food']\n",
      "Accumulated time spent: 2.9127 minutes\n",
      "9, Tone: Defensive, Food type: ['Hot dog stand', 'Indian']\n",
      "Accumulated time spent: 3.2936 minutes\n",
      "10, Tone: Demeaning, Food type: ['Italian', 'Japanese']\n",
      "Accumulated time spent: 3.6084 minutes\n",
      "11, Tone: Dismissive, Food type: ['Juice bar', 'Korean']\n",
      "Accumulated time spent: 3.8894 minutes\n",
      "12, Tone: Enthusiastic, Food type: ['Mediterranean', 'Mexican']\n",
      "Accumulated time spent: 4.2707 minutes\n",
      "13, Tone: Excited, Food type: ['Middle Eastern', 'Pizza parlor']\n",
      "Accumulated time spent: 4.5940 minutes\n",
      "14, Tone: Friendly, Food type: ['Pub', 'Ramen shop']\n",
      "Accumulated time spent: 4.9447 minutes\n",
      "15, Tone: Humorous, Food type: ['Sandwich shop', 'Seafood']\n",
      "Accumulated time spent: 5.3081 minutes\n",
      "16, Tone: Impersonal, Food type: ['Soul food', 'South American']\n",
      "Accumulated time spent: 5.6312 minutes\n",
      "17, Tone: Indifferent, Food type: ['Steakhouse', 'Sushi bar']\n",
      "Accumulated time spent: 5.9636 minutes\n",
      "18, Tone: Inquisitive, Food type: ['Taco shop', 'Tapas bar']\n",
      "Accumulated time spent: 6.2922 minutes\n",
      "19, Tone: Ironic, Food type: ['Tea house', 'Thai']\n",
      "Accumulated time spent: 6.6619 minutes\n",
      "20, Tone: Judgmental, Food type: ['Vegetarian', 'Vietnamese']\n",
      "Accumulated time spent: 6.9616 minutes\n",
      "21, Tone: Neutral, Food type: ['Wings joint', 'Bagel shop']\n",
      "Accumulated time spent: 7.2748 minutes\n",
      "22, Tone: Polite, Food type: ['Comfort food', 'Doughnut shop']\n",
      "Accumulated time spent: 7.6601 minutes\n",
      "23, Tone: Sarcastic, Food type: ['Family-style', 'Farm-to-table']\n",
      "Accumulated time spent: 7.9808 minutes\n",
      "24, Tone: Serious, Food type: ['Gluten-free', 'Ice cream parlor']\n",
      "Accumulated time spent: 8.3167 minutes\n",
      "25, Tone: Submissive, Food type: ['Lobster shack', 'Molecular gastronomy']\n",
      "Accumulated time spent: 8.6563 minutes\n",
      "26, Tone: Sympathetic, Food type: ['Pan-Asian', 'Plant-based']\n",
      "Accumulated time spent: 8.9765 minutes\n",
      "27, Tone: Understanding, Food type: ['Raw food', 'Vegan']\n",
      "Accumulated time spent: 9.3022 minutes\n",
      "28, Tone: Urgent, Food type: ['Waffle house', 'Winery']\n",
      "Accumulated time spent: 9.6255 minutes\n",
      "29, Tone: Poor English, Food type: ['Street food', 'Pop-up restaurant']\n",
      "Accumulated time spent: 9.9483 minutes\n"
     ]
    }
   ],
   "source": [
    "generation_config = genai.GenerationConfig(temperature=0.7, top_k=64)\n",
    "\n",
    "calib_data_raw = {}   # buffer to save Gemini output:\n",
    "i = 0\n",
    "err = 0\n",
    "start_time = time.time()\n",
    "while len(calib_data_raw) < 30:\n",
    "    tone = tones[i]\n",
    "    food_type = food_types[i*2:2*(i+1)]\n",
    "    try: \n",
    "        if tone not in calib_data_raw:\n",
    "            print(f\"{i}, Tone: {tone}, Food type: {str(food_type)}\")\n",
    "            input_text = prompts.restaurant_calib_data(tone=tone, food_type=food_type)\n",
    "            output = gemini.generate_content(input_text, generation_config=generation_config)\n",
    "            calib_data_raw[tone] = output.text   # save output\n",
    "            i += 1\n",
    "    except:   # may have internet error\n",
    "        err += 1\n",
    "        print(f\"{err} Internet error at: {i}\")\n",
    "    print(f\"Accumulated time spent: {(time.time() - start_time)/60:.4f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_lst = []\n",
    "for key, val in calib_data_raw.items():\n",
    "    val = val.replace(\"```\", \"\")   # remove sep symbols\n",
    "    val = val.replace(\"python\", \"\")   # remove sep symbols\n",
    "    chat_split = val.split(' = ')\n",
    "    try:\n",
    "        cur_lst = eval(chat_split[-1])\n",
    "        cur_lst = [{\"user_tone\": key, **item} for item in cur_lst]\n",
    "        chat_lst.extend(cur_lst)\n",
    "    except:\n",
    "        print(f\"failed at key value: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_data = pd.DataFrame(chat_lst)\n",
    "calib_data['user_input'] = np.where(calib_data['user'].isna(), calib_data['input'], calib_data['user'])\n",
    "calib_data['model_output'] = np.where(calib_data['AI assistant'].isna(), calib_data['output'], calib_data['AI assistant'])\n",
    "calib_data = calib_data[['user_tone', 'user_input', 'model_output']]\n",
    "# calib_data.isna().mean()\n",
    "now_timestamp = re.sub('(:|\\s|\\.)', '_', str(datetime.datetime.now()))\n",
    "calib_data.to_csv(f\"data/restaurant_chat_{now_timestamp}.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_tone</th>\n",
       "      <th>user_input</th>\n",
       "      <th>model_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>I'm looking for a place that's both kid-friendly and has a great selection of craft beers.</td>\n",
       "      <td>Kid-friendly and craft beers? [Restaurant Name] fits the bill. They have a dedicated kids' menu and an impressive tap list featuring local brews.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Authoritative</td>\n",
       "      <td>Hi there, I'm in the mood for a buffet restaurant with a sushi bar.</td>\n",
       "      <td>Sushi lovers, unite! [Restaurant Name] combines a delectable buffet with a fully stocked sushi bar. Dive into a culinary adventure that will satisfy your cravings for both variety and freshness.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Demeaning</td>\n",
       "      <td>I'm in the mood for something healthy and light.</td>\n",
       "      <td>For a healthy and refreshing Japanese meal, try [Restaurant Name]. They offer a variety of light dishes such as sashimi, salads, and steamed entrees.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>I'm looking for a place that serves up mouthwatering burgers and fries.</td>\n",
       "      <td>Burgers and fries, you say? [Restaurant Name] is your burger heaven. Their patties are juicy, their fries are crispy, and their milkshakes are the perfect complement.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>Ironic</td>\n",
       "      <td>I'm craving some Thai food that will make my taste buds dance.</td>\n",
       "      <td>Prepare for a culinary adventure at [Restaurant Name]. Their tantalizing dishes are a symphony of flavors that will awaken your senses and leave you craving more.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_tone  \\\n",
       "455      Sarcastic   \n",
       "52   Authoritative   \n",
       "202      Demeaning   \n",
       "460      Sarcastic   \n",
       "380         Ironic   \n",
       "\n",
       "                                                                                     user_input  \\\n",
       "455  I'm looking for a place that's both kid-friendly and has a great selection of craft beers.   \n",
       "52                          Hi there, I'm in the mood for a buffet restaurant with a sushi bar.   \n",
       "202                                            I'm in the mood for something healthy and light.   \n",
       "460                     I'm looking for a place that serves up mouthwatering burgers and fries.   \n",
       "380                              I'm craving some Thai food that will make my taste buds dance.   \n",
       "\n",
       "                                                                                                                                                                                           model_output  \n",
       "455                                                   Kid-friendly and craft beers? [Restaurant Name] fits the bill. They have a dedicated kids' menu and an impressive tap list featuring local brews.  \n",
       "52   Sushi lovers, unite! [Restaurant Name] combines a delectable buffet with a fully stocked sushi bar. Dive into a culinary adventure that will satisfy your cravings for both variety and freshness.  \n",
       "202                                               For a healthy and refreshing Japanese meal, try [Restaurant Name]. They offer a variety of light dishes such as sashimi, salads, and steamed entrees.  \n",
       "460                              Burgers and fries, you say? [Restaurant Name] is your burger heaven. Their patties are juicy, their fries are crispy, and their milkshakes are the perfect complement.  \n",
       "380                                  Prepare for a culinary adventure at [Restaurant Name]. Their tantalizing dishes are a symphony of flavors that will awaken your senses and leave you craving more.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "display(calib_data.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95505c81c5d049108ba31c83a10e8470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac47853e0dce4289b8aafb457f2b42d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tctsung/gemini_restaurant_chat/commit/ad5409373efa4d37934ba38b67d2e2643ce60c59', commit_message='Upload dataset', commit_description='', oid='ad5409373efa4d37934ba38b67d2e2643ce60c59', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push data onto huggingface:\n",
    "from datasets import Dataset\n",
    "\n",
    "finetuning_dataset = Dataset.from_pandas(calib_data)\n",
    "finetuning_dataset.push_to_hub(\"tctsung/tctsung/chat_restaurant_recommendation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\hf\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\anaconda3\\envs\\hf\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Open ELM model from Apple\n",
    "model_id = \"apple/OpenELM-270M-Instruct\"\n",
    "openelm_270m = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", force_download=True, trust_remote_code=True, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/OpenELM-270M-Instruct\\\\tokenizer_config.json',\n",
       " '../models/OpenELM-270M-Instruct\\\\special_tokens_map.json',\n",
       " '../models/OpenELM-270M-Instruct\\\\tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_folder = \"../models/OpenELM-270M-Instruct\"\n",
    "openelm_270m.save_pretrained(save_folder)\n",
    "tokenizer.save_pretrained(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "generation_config = GenerationConfig(\n",
    "\tmax_new_tokens=100, \n",
    "\tdo_sample=True,        # sampling or not, use greedy decoding if False\n",
    "\ttemperature = 0.95\n",
    "\t# top_k = 30,          # default 50\n",
    "\t# top_p = 0.3          # default 1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\hf\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load saved models:\n",
    "local_path = \"../models/OpenELM-270M-Instruct\"\n",
    "openelm_270m = AutoModelForCausalLM.from_pretrained(local_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_path, trust_remote_code=True, padding='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add padding token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 32000, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      " \n",
      "\n",
      "I'm craving some good ol' American comfort food. Any suggestions?[/INST]\n",
      "\n",
      "## Favorite American recipes and dishes\n",
      "\n",
      " - Pulled Pork Sandwich (made by: TJ's): Roast beef, barbeque sauce, American mustard, onions, pickles, lettuce, and a bun. [/INST]\n",
      "\n",
      "## Favourite American food culture moments\n",
      "\n",
      "* [This family](https://youtu.be/9LKrRuTxbqY\n"
     ]
    }
   ],
   "source": [
    "input_text = \"I'm craving some good ol' American comfort food. Any suggestions?\"\n",
    "device = torch.device('cuda')\n",
    "output_text, generation_time = helper.generate(openelm_270m, tokenizer, input_text, generation_config, device=torch.device('cuda'))\n",
    "print(\"---------------\\n\",output_text.split('<</SYS>>')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1979203a7ded4742874fd54a47deae69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\hf\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--meta-llama--Meta-Llama-3-8B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245c0131335448678a65d42c43dd3ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb3ce17f3fc4944b375a5f340973445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bccf1d7f6840d9baaa059e61d035e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03bf8ab5262434b9b08b5334f5f185c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35685a1d9e24d6197681a237d0d0db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39fc1658cd448febb7d4c3005ec3fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:  43%|####2     | 2.13G/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fea19881bc345c8ba2c5748a0f8dd00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f0b8f7446a49848a4ea269a3f63c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e825b90df604f9997a8ae0c37a859b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b61106965b8410889e56ba67711fbd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c67cee58ea54cd8952831cb03b02149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load llama3 model:\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llama3 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/Llama-3-8B-Instruct\\\\tokenizer_config.json',\n",
       " '../models/Llama-3-8B-Instruct\\\\special_tokens_map.json',\n",
       " '../models/Llama-3-8B-Instruct\\\\tokenizer.json')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_folder = \"../models/Llama-3-8B-Instruct\"\n",
    "llama3.save_pretrained(save_folder, safe_serialization=False)\n",
    "tokenizer.save_pretrained(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You can't move a model that has some modules offloaded to cpu or disk.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm craving some good ol\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m American comfort food. Any suggestions?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m output_text, generation_time \u001b[38;5;241m=\u001b[39m \u001b[43mhelper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllama3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m output_text\n",
      "File \u001b[1;32md:\\code\\quantization\\func\\helper.py:12\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(model, tokenizer, input_text, generation_config, device, max_length)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(model, tokenizer, input_text, generation_config, device, max_length: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m):\n\u001b[1;32m---> 12\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meval()   \u001b[38;5;66;03m# turn to inference mode\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m prompts\u001b[38;5;241m.\u001b[39mget_llama_chat_prompt(input_text)\n\u001b[0;32m     14\u001b[0m     stime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\hf\\Lib\\site-packages\\accelerate\\big_modeling.py:455\u001b[0m, in \u001b[0;36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 455\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: You can't move a model that has some modules offloaded to cpu or disk."
     ]
    }
   ],
   "source": [
    "input_text = \"I'm craving some good ol' American comfort food. Any suggestions?\"\n",
    "device = torch.device('cuda')\n",
    "output_text, generation_time = helper.generate(llama3, tokenizer, input_text, generation_config, device=torch.device('cuda'))\n",
    "output_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
