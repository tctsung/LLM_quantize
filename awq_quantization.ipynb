{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected: True\n",
      "GPU device count: 1\n"
     ]
    }
   ],
   "source": [
    "# basics\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime   # \n",
    "import re\n",
    "\n",
    "# load self-written func\n",
    "sys.path.append(\"func/\")      # add path \n",
    "import helper\n",
    "import prompts\n",
    "\n",
    "# load env variables\n",
    "from dotenv import dotenv_values\n",
    "from huggingface_hub import HfApi\n",
    "ENV_VAR = dotenv_values(\"../env/.env\")\n",
    "Gemini_key = ENV_VAR['Gemini_key']\n",
    "HF_key = ENV_VAR['HF_key']\n",
    "from huggingface_hub import login\n",
    "login(token=HF_key)                 # log-in for HF\n",
    "\n",
    "# DS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# DL\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # HF models\n",
    "import google.generativeai as genai     # Gemini model\n",
    "print(f\"GPU detected: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "# AWQ quantization:\n",
    "from awq import AutoAWQForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'prompts' from 'd:\\\\code\\\\LLM_quantize\\\\func\\\\prompts.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload module if modified\n",
    "from importlib import reload\n",
    "reload(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\hf\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'left'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tiny llama model from HF API:\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tinyllama = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/TinyLlama-1.1B-chat-v1.0\\\\tokenizer_config.json',\n",
       " '../models/TinyLlama-1.1B-chat-v1.0\\\\special_tokens_map.json',\n",
       " '../models/TinyLlama-1.1B-chat-v1.0\\\\tokenizer.json')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model to local:\n",
    "save_folder = \"../models/TinyLlama-1.1B-chat-v1.0\"\n",
    "tinyllama.save_pretrained(save_folder)\n",
    "tokenizer.save_pretrained(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic performance of the model:\n",
    "from transformers import pipeline\n",
    "# set inference config:\n",
    "from transformers import GenerationConfig\n",
    "generation_config = GenerationConfig(\n",
    "\tmax_new_tokens=100, \n",
    "\tdo_sample=True,        # sampling or not, use greedy decoding if False\n",
    "\ttemperature = 1,\n",
    "\ttop_k = 30,          # default 50\n",
    "\t# top_p = 0.3          # default 1.0\n",
    ")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Hey, can you recommend me sushi restaurant in new york city?\"}\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "generator = pipeline('text-generation', model=save_folder, tokenizer=save_folder, do_sample=True, num_return_sequences=3, generation_config = generation_config)\n",
    "output = generator(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7min 33s\n",
      "Wall time: 57.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "output = generator([input_text, input_text, input_text], return_tensors=\"pt\", num_return_sequences=1, generation_config = generation_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models to autoawq module:\n",
    "model_path = \"../models/TinyLlama-1.1B-chat-v1.0\"\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# parameters:\n",
    "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  \n",
      "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
      "Please ensure that your responses are socially unbiased and positive in nature.\n",
      "</s>\n",
      "<|user|>\n",
      "I'm looking for a restaurant that caters to vegetarians and vegans.</s>\n",
      "<|assistant|>\n",
      "For vegetarian and vegan options, check out [Restaurant Name]. Their menu offers a wide variety of plant-based dishes that are both flavorful and satisfying.</s>\n"
     ]
    }
   ],
   "source": [
    "# prepare calib_data:\n",
    "calib_df = pd.read_csv('data/restaurant_chat_2024-05-11_21_38_18_603307.csv')\n",
    "calib_data = []\n",
    "for index, row in calib_df.iterrows():\n",
    "    if index % 2 == 0:\n",
    "        sys_msg = prompts.SYSTEM_MESSAGES['restaurant']\n",
    "    else:\n",
    "        sys_msg = prompts.SYSTEM_MESSAGES['default']\n",
    "    msg = [\n",
    "        {\"role\": \"system\", \"content\": sys_msg,},\n",
    "        {\"role\": \"user\",  \"content\": row['user_input']},\n",
    "        {'role': 'assistant', 'content': row['model_output']}\n",
    "    ]\n",
    "    input_text = tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=False)\n",
    "    calib_data.append(input_text.strip())\n",
    "# input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(calib_data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AWQ: 100%|██████████| 22/22 [05:50<00:00, 15.95s/it]\n"
     ]
    }
   ],
   "source": [
    "# Quantize\n",
    "model.quantize(tokenizer, quant_config=quant_config, calib_data=calib_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/TinyLlama-1.1B-chat-v1.0-awq\\\\tokenizer_config.json',\n",
       " '../models/TinyLlama-1.1B-chat-v1.0-awq\\\\special_tokens_map.json',\n",
       " '../models/TinyLlama-1.1B-chat-v1.0-awq\\\\tokenizer.json')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model to local\n",
    "output_dir = '../models/TinyLlama-1.1B-chat-v1.0-awq'\n",
    "model.save_quantized(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model back:\n",
    "tinyllama_awq = AutoModelForCausalLM.from_pretrained(\n",
    "    '../models/TinyLlama-1.1B-chat-v1.0-awq',\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/tctsung/TinyLlama-1.1B-chat-v1.0-awq', endpoint='https://huggingface.co', repo_type='model', repo_id='tctsung/TinyLlama-1.1B-chat-v1.0-awq')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "create_repo(\"tctsung/TinyLlama-1.1B-chat-v1.0-awq\", repo_type=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09e672d3e8a440a9fe052f607535360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/766M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/tctsung/TinyLlama-1.1B-chat-v1.0-awq/commit/322c9a3ff9a3cdd9380fba2e3c28bdfd266e7207', commit_message='Upload folder using huggingface_hub', commit_description='', oid='322c9a3ff9a3cdd9380fba2e3c28bdfd266e7207', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path='../models/TinyLlama-1.1B-chat-v1.0-awq',\n",
    "    repo_id=\"tctsung/TinyLlama-1.1B-chat-v1.0-awq\",\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "x = helper.generate(\n",
    "    tinyllama_awq, tokenizer, \"Hi how are you\", generation_config, torch.device('cuda')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'left'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  \n",
      "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
      "Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "Hi how are you[/INST] \n",
      "\n",
      "Greetings! I am a dedicated and responsible assistant. I ensure that my responses are truthful, honest, and socially unbiased. \n",
      "\n",
      "Please, do not share any harmful, toxic or unhelpful content regarding any personal information. Also, I recommend you keep your answers simple and straightforward, avoiding technical jargon.\n",
      "\n",
      "Wishing you warm regards. [INST]<<SYS>> \n",
      "<|user|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
